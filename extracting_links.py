# -*- coding: utf-8 -*-
"""extracting_links.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PjCrh_Icx4nb-smCJJgFNielOQnSbFH0
"""

import requests
from bs4 import *
from urllib.parse import urljoin

def extract_urls(url):
  if(url.startswith('www.')):
    url = "https://" + url
    print(url)
  response = requests.get(url)
  soup = BeautifulSoup(response.content, 'html.parser')
  urls = set()

  for a in soup.find_all('a'):
    href = a.get('href')
    if href:
      absolute_url = urljoin(url, href)
      if "wiki" in absolute_url and (absolute_url.startswith('https://en.wikipedia.org' or absolute_url.startswith('/wiki/'))):
        urls.add(absolute_url)
      elif "curlie.org" in absolute_url and (absolute_url.startswith('https://www.curlie.org/')):
        urls.add(absolute_url)

  return urls

def recursive_extract_urls(url, found_urls, depth=1, max_depth=3, max_links=100_000):
  if depth > max_depth or len(found_urls) >= max_links:
    return
  found_urls.add(str(url))
  urls = extract_urls(url)
  for url in urls:
    recursive_extract_urls(url, found_urls, depth+1, max_depth, max_links)

# extracting wikipedia links recursively (up to n links):
wiki_urls = set()
max_links=25
recursive_extract_urls("www.wikipedia.org", wiki_urls, max_links=max_links)
print(f"got {max_links} urls, extracted from: 'www.wikipedia.org':")
for url in wiki_urls:
  print(url)

# extracting curlie links recursively (up to n links):
curlie_urls = set()
max_links = 25
recursive_extract_urls("www.curlie.org", curlie_urls, max_links=max_links)
print("urls extracted from: 'www.curlie.org':")
for url in curlie_urls:
  print(url)